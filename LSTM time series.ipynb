{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math\n",
    "from tensorflow.contrib.rnn import LSTMCell, DropoutWrapper, MultiRNNCell\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"PRSA_data_2010.1.1-2014.12.31.csv\")\n",
    "data.index = data.apply(lambda x:datetime(x.year, x.month, x.day, x.hour), axis=1)\n",
    "\n",
    "data.drop(labels=[\"year\", \"month\", \"day\", \"hour\"], axis=1, inplace=True)\n",
    "data.rename(columns={\"pm2.5\":\"pm25\"}, inplace=True)\n",
    "data = data[pd.isnull(data[\"pm25\"]) == False]\n",
    "data = pd.concat([data, pd.get_dummies(data.cbwd)], axis=1)\n",
    "data.drop(labels=[\"cbwd\", \"No\"], axis=1, inplace=True)\n",
    "ts = pd.date_range(start=data.index.min(), end=data.index.max(), freq=\"H\")\n",
    "data = data.reindex(index=ts, method=\"pad\")\n",
    "\n",
    "\n",
    "data[\"pm25_next\"] = data.pm25.shift(-1)\n",
    "data = data[pd.isnull(data.pm25_next)==False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = data.pm25_next.values\n",
    "X = data.drop(labels=\"pm25_next\", axis=1).values\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_set_with_timestep(X, Y, time_steps):\n",
    "    X_with_t = np.zeros((X.shape[0] - time_steps, time_steps, X.shape[1]))\n",
    "    Y_with_t = np.zeros(((Y.shape[0] - time_steps),1))\n",
    "    # sliding window with time steps width of for the sample\n",
    "    # in this case the y would be the last y of the sample (because we shifted that earlier)\n",
    "    \n",
    "    for i in range(X_with_t.shape[0]):\n",
    "        to_i = i + time_steps\n",
    "        if to_i >= X.shape[0]:\n",
    "            break;\n",
    "        X_with_t[i,...] = np.vstack(X[i:to_i,:])\n",
    "        Y_with_t[i,...] = Y[to_i-1]\n",
    "    return (X_with_t, Y_with_t)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_and_train(time_steps, num_units, keep_prob, do_summaries, batch_size, no_of_epochs, save_mode=False):\n",
    "    global X, Y\n",
    "\n",
    "    #break down data for train and test\n",
    "    #Here we take into consideration the time_steps\n",
    "    size_of_test = X.shape[0]//10\n",
    "    size_of_train = X.shape[0] - X.shape[0]//10\n",
    "    size_of_test = size_of_test + size_of_train%time_steps -1\n",
    "    size_of_train = X.shape[0] - size_of_test\n",
    "    cut_off = size_of_train - 1\n",
    "\n",
    "    X_train, Y_train = X[0:cut_off,:], Y[0:cut_off]\n",
    "    X_test, Y_test = X[cut_off:,:], Y[cut_off::]\n",
    "\n",
    "    #Get the min and max for all data\n",
    "    X_min, X_max = np.min(X, axis=0), np.max(X, axis=0)\n",
    "\n",
    "    #Standardize data\n",
    "    X_train = (X_train-X_min)/(X_max-X_min)\n",
    "    X_test = (X_test-X_min)/(X_max-X_min)\n",
    "\n",
    "    assert(np.all(np.max(X_train, axis=0) == 1.0))\n",
    "    assert(np.all(np.min(X_train, axis=0) == 0.0))\n",
    "\n",
    "    X_train_v = X_train[0:50, :]\n",
    "    Y_train_v = Y_train[0:50]\n",
    "    X_train_ut, Y_train_ut = create_set_with_timestep(X_train_v, Y_train_v, time_steps)\n",
    "\n",
    "    #assert(np.all(X_train_ut[25,0,:] == X_train_v[25,:]))\n",
    "    #assert(Y_train_ut[24] == Y_train_v[24+time_steps-1])\n",
    "\n",
    "    del X_train_ut, X_train_v\n",
    "    gc.collect()\n",
    "\n",
    "    X_train, Y_train = create_set_with_timestep(X_train, Y_train, time_steps)\n",
    "    X_test, Y_test = create_set_with_timestep(X_test, Y_test, time_steps)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    graph = tf.Graph()\n",
    "    X_shape = X_train.shape\n",
    "    Y_shape = Y_train.shape\n",
    "\n",
    "\n",
    "    with graph.as_default():\n",
    "\n",
    "\n",
    "        Xt = tf.placeholder(shape=[None, X_shape[1], X_shape[2]], dtype=tf.float32)\n",
    "        Yt = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "        b_size = tf.shape(Xt)[0]\n",
    "        keep_prob_t = tf.placeholder(tf.float32)\n",
    "        #I do a basic now and look at the graph\n",
    "        lstm_cells = list()\n",
    "        for u in range(len(num_units)):\n",
    "            lstm_cells.append(\n",
    "                DropoutWrapper(LSTMCell(num_units=num_units[u], state_is_tuple=True), \n",
    "                               input_keep_prob=keep_prob_t,\n",
    "                               output_keep_prob=keep_prob_t))\n",
    "\n",
    "        lstm_cells = MultiRNNCell(lstm_cells, state_is_tuple=True)\n",
    "        initial_states = lstm_cells.zero_state(b_size, dtype=tf.float32)\n",
    "        outputs, states = tf.nn.dynamic_rnn(lstm_cells, inputs=Xt, initial_state=initial_states)\n",
    "\n",
    "\n",
    "        fc_final = fully_connected(inputs=outputs[:,-1,:], activation_fn=None, num_outputs=1)\n",
    "        loss = tf.losses.mean_squared_error(labels=Yt, predictions=fc_final)\n",
    "        loss_rmse = tf.sqrt(loss)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        gradients = optimizer.compute_gradients(loss)\n",
    "\n",
    "\n",
    "        optimize = optimizer.apply_gradients(gradients)\n",
    "        if do_summaries:\n",
    "            tf.summary.scalar(\"loss_rmse\", tf.sqrt(loss))\n",
    "            merge_summaries = tf.summary.merge_all()\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "\n",
    "        no_of_iterations = X_train.shape[0]//batch_size + 1\n",
    "        report_cycle = 100\n",
    "        if do_summaries:\n",
    "            summarize_train = tf.summary.FileWriter(logdir=\"log//train\", graph=graph)\n",
    "            summarize_test = tf.summary.FileWriter(logdir=\"log//test\", graph=graph)\n",
    "        sess.run(init)\n",
    "        counter = 1\n",
    "        loss_list = dict()\n",
    "        \n",
    "        for epoch in range(no_of_epochs):\n",
    "            for n in range(no_of_iterations):\n",
    "                train_idxs = np.random.choice(np.arange(X_train.shape[0]), batch_size)\n",
    "\n",
    "                if do_summaries: \n",
    "                    _, loss_val, train_summary  = sess.run([optimize, loss_rmse, merge_summaries],\n",
    "                                                           feed_dict={Xt:X_train[train_idxs,...], \n",
    "                                                           Yt:Y_train[train_idxs,...],\n",
    "                                                           keep_prob_t:keep_prob})\n",
    "                else:\n",
    "                    _, loss_val = sess.run([optimize, loss_rmse], feed_dict={Xt:X_train[train_idxs,...], \n",
    "                                                             Yt:Y_train[train_idxs,...],\n",
    "                                                             keep_prob_t:keep_prob})\n",
    "                \n",
    "\n",
    "                if n%report_cycle==0:\n",
    "                    if do_summaries:\n",
    "                        summarize_train.add_summary(train_summary, counter)\n",
    "                        loss_test_val, test_summary = sess.run([loss_rmse, merge_summaries], \n",
    "                                                               feed_dict={Xt:X_test[0:50,...], \n",
    "                                                               Yt:Y_test[0:50,...],\n",
    "                                                               keep_prob_t:1.0})\n",
    "                        summarize_test.add_summary(test_summary, counter)\n",
    "\n",
    "                    else:\n",
    "                        loss_test_val = sess.run(loss_rmse, feed_dict={Xt:X_test[0:50,...], \n",
    "                                                         Yt:Y_test[0:50,...],\n",
    "                                                         keep_prob_t:1.0})\n",
    "                    loss_list[counter] = {\"loss_train\":float(loss_val), \n",
    "                                          \"loss_test\":float(loss_test_val)}\n",
    "                    counter += 1\n",
    "                    print(\"\\repoch={:<10d}--Iterations={:<10d}--loss train rmse={:<.3f}--loss test rmse={:<.3f}\".\\\n",
    "                          format(epoch, n, math.sqrt(loss_val), math.sqrt(loss_test_val)), end =\" \")\n",
    "    return(loss_list, graph)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_units_par = [[64, 64]]\n",
    "keep_prob_par = [0.95, 1]\n",
    "time_steps_par = np.random.randint(19, 32, 5)\n",
    "\n",
    "par_combinations = [(t, n, p) for t in time_steps_par for n in num_units_par for p in keep_prob_par]\n",
    "\n",
    "results = dict()\n",
    "for i, par in enumerate(par_combinations):\n",
    "    time_steps, num_units, keep_prob = par\n",
    "      \n",
    "    print(\"Doing {}\".format((time_steps, num_units, keep_prob)))\n",
    "    loss_list, _ = build_and_train(time_steps, num_units, keep_prob, False, 50, 200)\n",
    "    results[i] = {\"time_steps\":int(time_steps), \"layers\":num_units, \"keep_prob\":keep_prob, \"losses\":loss_list}\n",
    "\n",
    "with open(\"log_results//results_fine.json\", \"w\") as op:\n",
    "    json.dump(results, op)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_loss_stats(results, loss_type, loss_avg_window, diff_avg_window):\n",
    "    n_of_items = len(results[\"losses\"])\n",
    "    losses_arr = np.ndarray([n_of_items])\n",
    "    for i in range(n_of_items):\n",
    "        losses_arr[i] = results[\"losses\"][i+1][loss_type]\n",
    "    avg_l_diff = np.average(np.diff(losses_arr)[-diff_avg_window::])\n",
    "    std_l_diff = np.std(np.diff(losses_arr)[-diff_avg_window::])\n",
    "    avg_l = np.average(losses_arr[-loss_avg_window::])\n",
    "    std_l = np.std(losses_arr[-loss_avg_window::])\n",
    "    return ([avg_l_diff, std_l_diff, avg_l, std_l])\n",
    "\n",
    "\n",
    "with open(\"log_results//results_fine.json\", \"r\") as op:\n",
    "    result = json.load(op)\n",
    "    \n",
    "results_df = pd.DataFrame(columns=[[\"time_steps\", \"layers\", \"keep_prob\", \n",
    "                                    \"loss_train_grad_avg\", \"loss_train_grad_std\", \n",
    "                                    \"loss_train_avg\", \"loss_train_std\",\n",
    "                                    \"loss_test_grad_avg\", \"loss_test_grad_std\", \n",
    "                                    \"loss_test_avg\", \"loss_test_std\"]])\n",
    "diff_avg_window = 5\n",
    "loss_avg_window = 5\n",
    "n_of_results = len(results)\n",
    "for ir in range(n_of_results):\n",
    "      results_df.loc[ir] = [results[ir][\"time_steps\"], results[ir][\"layers\"], results[ir][\"keep_prob\"]] +\\\n",
    "        get_loss_stats(results[ir], \"loss_train\", loss_avg_window, diff_avg_window) +\\\n",
    "        get_loss_stats(results[ir], \"loss_test\", loss_avg_window, diff_avg_window) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_df.sort_values(by=[\"loss_test_avg\", \"loss_test_grad_avg\"], ascending=[True, True]).head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=199       --Iterations=700       --loss train rmse=2.884 "
     ]
    }
   ],
   "source": [
    "#break down data for train and test\n",
    "#Here we take into consideration the time_steps\n",
    "########################################\n",
    "time_steps = 23\n",
    "num_units = [64, 64]\n",
    "keep_prob_t = 1.0\n",
    "batch_size = 50\n",
    "no_of_epochs = 200\n",
    "########################################\n",
    "\n",
    "\n",
    "size_of_test = X.shape[0]//5\n",
    "size_of_train = X.shape[0] - X.shape[0]//5\n",
    "size_of_test = size_of_test + size_of_train%time_steps -1\n",
    "size_of_train = X.shape[0] - size_of_test\n",
    "cut_off = size_of_train - 1\n",
    "\n",
    "X_train, Y_train = X[0:cut_off,:], Y[0:cut_off]\n",
    "X_test, Y_test = X[cut_off:,:], Y[cut_off::]\n",
    "\n",
    "#Get the min and max for all data\n",
    "X_min, X_max = np.min(X, axis=0), np.max(X, axis=0)\n",
    "\n",
    "#Standardize data\n",
    "X_train = (X_train-X_min)/(X_max-X_min)\n",
    "X_test = (X_test-X_min)/(X_max-X_min)\n",
    "\n",
    "X_train_v = X_train[0:50, :]\n",
    "Y_train_v = Y_train[0:50]\n",
    "X_train_ut, Y_train_ut = create_set_with_timestep(X_train_v, Y_train_v, time_steps)\n",
    "\n",
    "\n",
    "del X_train_ut, X_train_v\n",
    "gc.collect()\n",
    "\n",
    "X_train, Y_train = create_set_with_timestep(X_train, Y_train, time_steps)\n",
    "X_test, Y_test = create_set_with_timestep(X_test, Y_test, time_steps)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "X_shape = X_train.shape\n",
    "Y_shape = Y_train.shape\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "\n",
    "    Xt = tf.placeholder(shape=[None, X_shape[1], X_shape[2]], dtype=tf.float32, name=\"Xt\")\n",
    "    Yt = tf.placeholder(shape=[None, 1], dtype=tf.float32, name=\"Yt\")\n",
    "    b_size = tf.shape(Xt)[0]\n",
    "\n",
    "    #I do a basic now and look at the graph\n",
    "    lstm_cells = list()\n",
    "    for u in range(len(num_units)):\n",
    "        lstm_cells.append(\n",
    "            DropoutWrapper(LSTMCell(num_units=num_units[u], state_is_tuple=True), \n",
    "                           input_keep_prob=keep_prob_t,\n",
    "                           output_keep_prob=keep_prob_t))\n",
    "\n",
    "    lstm_cells = MultiRNNCell(lstm_cells, state_is_tuple=True)\n",
    "    initial_states = lstm_cells.zero_state(b_size, dtype=tf.float32)\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cells, inputs=Xt, initial_state=initial_states)\n",
    "\n",
    "\n",
    "    fc_final = fully_connected(inputs=outputs[:,-1,:], activation_fn=None, num_outputs=1)\n",
    "    loss = tf.losses.mean_squared_error(labels=Yt, predictions=fc_final)\n",
    "    loss_rmse = tf.sqrt(loss)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "\n",
    "\n",
    "    optimize = optimizer.apply_gradients(gradients)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    vars_to_save = graph.get_collection(\"trainable_variables\")\n",
    "    saver = tf.train.Saver(var_list=vars_to_save)\n",
    "\n",
    "    no_of_iterations = X_train.shape[0]//batch_size + 1\n",
    "    report_cycle = 100\n",
    "    sess.run(init)\n",
    "    counter = 1\n",
    "\n",
    "    for epoch in range(no_of_epochs):\n",
    "        \n",
    "        for n in range(no_of_iterations):\n",
    "            train_idxs = np.random.choice(np.arange(X_train.shape[0]), batch_size)\n",
    "            _, loss_val = sess.run([optimize, loss_rmse], feed_dict={Xt:X_train[train_idxs,...], \n",
    "                                                                     Yt:Y_train[train_idxs,...]})\n",
    "\n",
    "            if n%report_cycle==0:\n",
    "      \n",
    "                counter += 1\n",
    "                print(\"\\repoch={:<10d}--Iterations={:<10d}--loss train rmse={:<.3f}\".\\\n",
    "                      format(epoch, n, math.sqrt(loss_val)), end =\" \")\n",
    "        if epoch % 150==0:\n",
    "            saver.save(sess, \"model//lstm\", global_step=150)\n",
    "        elif epoch % 180==0:\n",
    "            saver.save(sess, \"model//lstm\", global_step=180)\n",
    "       \n",
    "    saver.save(sess, \"model//lstm\", global_step=200)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
