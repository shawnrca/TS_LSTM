{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math\n",
    "from tensorflow.contrib.rnn import LSTMCell, DropoutWrapper, MultiRNNCell\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"PRSA_data_2010.1.1-2014.12.31.csv\")\n",
    "data.index = data.apply(lambda x:datetime(x.year, x.month, x.day, x.hour), axis=1)\n",
    "\n",
    "data.drop(columns=[\"year\", \"month\", \"day\", \"hour\"], inplace=True)\n",
    "data.rename(columns={\"pm2.5\":\"pm25\"}, inplace=True)\n",
    "data = data[data[\"pm25\"].isna() == False]\n",
    "data = pd.concat([data, pd.get_dummies(data.cbwd)], axis=1)\n",
    "data.drop(columns=[\"cbwd\", \"No\"], inplace=True)\n",
    "ts = pd.date_range(start=data.index.min(), end=data.index.max(), freq=\"H\")\n",
    "data = data.reindex(index=ts, method=\"pad\")\n",
    "\n",
    "\n",
    "# Copy the pm25 and rename it to pm25\n",
    "#we can use this as a feture\n",
    "data[\"pm25_prev\"] = data.pm25\n",
    "\n",
    "#We do a shift here so that y for each sample is really the y for the next\n",
    "data.pm25 = data.pm25.shift(-1)\n",
    "data = data[data[\"pm25\"].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x000000000F4A9BA8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\srajabza\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 694, in __del__\n",
      "    if self._session is not None:\n",
      "AttributeError: 'Session' object has no attribute '_session'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = data.pm25.values\n",
    "X = data.drop(columns=\"pm25\").values\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (39408, 11) ----- Y_train shape: (39408,)-------\n"
     ]
    }
   ],
   "source": [
    "time_steps = 24\n",
    "\n",
    "#break down data for train and test\n",
    "#Here we take into consideration the time_steps\n",
    "size_of_test = X.shape[0]//10\n",
    "size_of_train = X.shape[0] - X.shape[0]//10\n",
    "size_of_test = size_of_test + size_of_train%time_steps -1\n",
    "size_of_train = X.shape[0] - size_of_test\n",
    "cut_off = size_of_train - 1\n",
    "\n",
    "X_train, Y_train = X[0:cut_off,:], Y[0:cut_off]\n",
    "X_test, Y_test = X[cut_off:,:], Y[cut_off::]\n",
    "\n",
    "#Get the min and max for all data\n",
    "X_min, X_max = np.min(X, axis=0), np.max(X, axis=0)\n",
    "\n",
    "#Standardize data\n",
    "X_train = (X_train-X_min)/(X_max-X_min)\n",
    "X_test = (X_test-X_min)/(X_max-X_min)\n",
    "\n",
    "assert(np.all(np.max(X_train, axis=0) == 1.0))\n",
    "assert(np.all(np.min(X_train, axis=0) == 0.0))\n",
    "\n",
    "print(\"X_train shape: {} ----- Y_train shape: {}-------\".format(X_train.shape, Y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_with_timestep(X, Y, time_steps):\n",
    "    X_with_t = np.zeros((X.shape[0] - time_steps, time_steps, X.shape[1]))\n",
    "    Y_with_t = np.zeros(((Y.shape[0] - time_steps),1))\n",
    "    # sliding window with time steps width of for the sample\n",
    "    # in this case the y would be the last y of the sample (because we shifted that earlier)\n",
    "    \n",
    "    for i in range(X_with_t.shape[0]):\n",
    "        to_i = i + time_steps\n",
    "        if to_i >= X.shape[0]:\n",
    "            break;\n",
    "        X_with_t[i,...] = np.vstack(X[i:to_i,:])\n",
    "        Y_with_t[i,...] = Y[to_i-1]\n",
    "    return (X_with_t, Y_with_t)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_v = X_train[0:50, :]\n",
    "Y_train_v = Y_train[0:50]\n",
    "X_train_ut, Y_train_ut = create_set_with_timestep(X_train_v, Y_train_v, time_steps)\n",
    "\n",
    "assert(np.all(X_train_ut[25,0,:] == X_train_v[25,:]))\n",
    "assert(Y_train_ut[24] == Y_train_v[47])\n",
    "\n",
    "del X_train_ut, X_train_v\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (39384, 24, 11) --- Y_train shape (39384, 1)\n",
      "X_test shape (4367, 24, 11) --- Y_test shape (4367, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = create_set_with_timestep(X_train, Y_train, time_steps)\n",
    "X_test, Y_test = create_set_with_timestep(X_test, Y_test, time_steps)\n",
    "\n",
    "print(\"X_train shape {} --- Y_train shape {}\\nX_test shape {} --- Y_test shape {}\".\n",
    "      format(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Development in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "X_shape = X_train.shape\n",
    "Y_shape = Y_train.shape\n",
    "num_units = 64\n",
    "batch_size = 50\n",
    "num_layers = 3\n",
    "with graph.as_default():\n",
    "    \n",
    "    X = tf.placeholder(shape=[batch_size, X_shape[1], X_shape[2]], dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=[batch_size, 1], dtype=tf.float32)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    #I do a basic now and look at the graph\n",
    "    lstm_cells = list()\n",
    "    for _ in range(num_layers):\n",
    "        lstm_cells.append(\n",
    "            DropoutWrapper(LSTMCell(num_units=num_units, state_is_tuple=True), \n",
    "                           input_keep_prob=keep_prob,\n",
    "                           output_keep_prob=keep_prob))\n",
    "    \n",
    "    lstm_cells = MultiRNNCell(lstm_cells, state_is_tuple=True)\n",
    "    initial_states = lstm_cells.zero_state(batch_size, dtype=tf.float32)\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cells, inputs=X, initial_state=initial_states)\n",
    "\n",
    "    \n",
    "    fc_final = fully_connected(inputs=outputs[:,-1,:], activation_fn=None, num_outputs=1)\n",
    "    loss = tf.losses.mean_squared_error(labels=Y, predictions=fc_final)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "    for i, gv in enumerate(gradients):\n",
    "        tf.summary.histogram(\"gradients_{}\".format(i), gv[0])\n",
    "    #gradients = [(tf.clip_by_value(g, tf.constant(-2000.0), tf.constant(100.0)), v) \\\n",
    "    #                     for g, v in gradients]\n",
    "    \n",
    "    optimize = optimizer.apply_gradients(gradients)\n",
    "    tf.summary.scalar(\"loss_train\", loss)\n",
    "    merge_summaries = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch=         7--Iterations=       700--loss train=8228.325--loss test=15162.192  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-c956276d6f68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m                                                    feed_dict={X:X_train[train_idxs,...], \n\u001b[0;32m     14\u001b[0m                                                               \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idxs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                                                             keep_prob:0.4})\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mreport_cycle\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0msummarize_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    no_of_epochs = 100000\n",
    "    no_of_iterations = X_train.shape[0]//batch_size + 1\n",
    "    report_cycle = 100\n",
    "    summarize_train = tf.summary.FileWriter(logdir=\"log\\\\train\", graph=graph)\n",
    "    summarize_test = tf.summary.FileWriter(logdir=\"log\\\\test\", graph=graph)\n",
    "    sess.run(init)\n",
    "    counter = 1\n",
    "    for epoch in range(no_of_epochs):\n",
    "        for n in range(no_of_iterations):\n",
    "            train_idxs = np.random.choice(np.arange(X_train.shape[0]), batch_size)\n",
    "            _, loss_val, train_summary  = sess.run([optimize, loss, merge_summaries], \n",
    "                                                   feed_dict={X:X_train[train_idxs,...], \n",
    "                                                              Y:Y_train[train_idxs,...],\n",
    "                                                            keep_prob:0.4})\n",
    "            if n%report_cycle==0:\n",
    "                summarize_train.add_summary(train_summary, counter)\n",
    "                loss_test_val, test_summary = sess.run([loss, merge_summaries], \n",
    "                                                   feed_dict={X:X_test[0:50,...], \n",
    "                                                              Y:Y_test[0:50,...],\n",
    "                                                              keep_prob:1.0})\n",
    "                summarize_test.add_summary(test_summary, counter)\n",
    "                counter += 1\n",
    "                print(\"\\r epoch={:10d}--Iterations={:10d}--loss train={:.3f}--loss test={:.3f}\".\\\n",
    "                      format(epoch, n, loss_val, loss_test_val), end =\" \")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
